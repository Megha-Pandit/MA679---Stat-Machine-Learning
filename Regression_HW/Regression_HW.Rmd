---
title: "02 Regression Homework"
author: "Megha Pandit"
date: "February 1, 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

1. Describe the null hypotheses to which the p-values given in Table 3.4 correspond. Explain what conclusions you can draw based on these p-values. Your explanation should be phrased in terms of sales, TV, radio, and newspaper, rather than in terms of the coefficients of the linear model.

_*The null hypotheses to which the p-values given in Table 3.4 correspond to are that TV, radio and newspaper advertising have no relationship with sales. More specifically,*_ $H_0^{(1)}: \beta_1 = 0$, $H_0^{(2)}: \beta_2 = 0$, $H_0^{(3)}: \beta_3 = 0$, _*where,*_ $\beta_1$ _*is the coefficient for TV advertising budget*_, $\beta_2$ _*is the coefficient for radio advertising budget and *_ $\beta_3$ _*is the coefficient for newspaper advertising budget. For TV and radio advertising, the p-values are very small indicating that we can reject their corresponding null hypotheses. Therefore, we can reject*_ $H_0^{(1)}$ _*and*_ $H_0^{(2)}$. _*However, the p-value for newspaper advertising is large indicating that we cannot reject the corresponding null hypothesis,*_ $H_0^{(3)}$. _*Therefore, from the p-value, newspaper advertising does not have an effect on sales.*_

$~$

2. Carefully explain the differences between the KNN classifier and KNN regression methods.

_*KNN classsifier is used to solve classification problems, usually where the response is qualitative with more than one or two levels. KNN regression method is used to solve regression problems where the response is either quantitative or qualitative in some cases. The major difference is in the mechanism of the two methods. KNN classifier identifies the neighborhood of*_ $x_0$ _*and then estimates*_ $f(x_0)$ _*as the average of all the responses in the neighborhood.*_

$~$

5. Consider the fitted values that result from performing linear regression without an intercept. In this setting, the ith fitted value takes the form $\hat{y}_i = x_i\hat\beta$ where,
$$\hat\beta = \frac{\sum_{i = 1}^{n}x_iy_i}{\sum_{i^{'} = 1}^{n}x_{i^{'}}^2}$$
Show that we can write
$$\hat{y}_i = \sum_{i^{'}=1}^{n}a_{i^{'}}y_{i^{'}}$$
what is $a_{i^{'}}$ ?

We have $\hat{y}_i = x_i\hat\beta$ _*and*_ $\hat\beta = (\sum_{i = 1}^{n}x_iy_i) / (\sum_{i^{'} = 1}^{n}x_{i^{'}}^2)$ therefore,
$$\hat{y}_i = x_i\frac{\sum_{i = 1}^{n}x_iy_i}{\sum_{i^{'} = 1}^{n}x_{i^{'}}^2} = \frac{\sum_{i = 1}^{n}x_i\frac{x_i}{n}y_i}{\sum_{i^{'} = 1}^{n}x_{i^{'}}^2}$$
$$\hat{y}_i = \sum_{i^{'} = 1}^n(\frac{\frac{{x_{i^{'}}}^2y_i}{n}}{x_{i^{'}}^2}) = \sum_{i^{'} = 1}^{n}\frac{1}{n}y_{i^{'}} = \sum_{i^{'} = 1}^{n}a_{i^{'}}y_{i^{'}}$$
Therefore, $$a_{i^{'}} = \frac{1}{n}$$

$~$

6. Using (3.4), argue that in the case of simple linear regression, the least squares line always passes through the point (¯x, ¯y).

Substituting $\bar{x}$ for x and $\bar{y}$ for y in the least squares equation, we get:
$$\bar{y} = \hat\beta_0 + \hat\beta_1\bar{x}$$
From 3.4, we have $\hat\beta_0 = \bar{y} - \hat\beta_1\bar{x}$
Therefore, $\bar{y} = \bar{y} - \hat\beta_1\bar{x} + \hat\beta_1\bar{x} = \bar{y}$
The above implies that the least squares line always passes through the point $(\bar{x},\bar{y})$

$~$

11.In this problem we will investigate the t-statistic for the null hypothesis H0 : ?? = 0 in simple linear regression without an intercept. To begin, we generate a predictor x and a response y as follows.

```{r}
set.seed (1)
x=rnorm (100)
y=2*x+rnorm (100)
```
(a) Perform a simple linear regression of y onto x, without an intercept. Report the coefficient estimate ^??, the standard error of this coefficient estimate, and the t-statistic and p-value associated with the null hypothesis H0 : ?? = 0. Comment on these results. (You can perform regression without an intercept using the command lm(y???x+0).)

```{r}
fit1 <- lm(y~x + 0)
summary(fit1)
```
_*The coefficient estimate*_ $\hat\beta$ _*is 1.9939, the standard error of*_ $\hat\beta$ _*is 0.1065, the t-statistic is 18.73, and the p-value is <2e-16. The null hypothesis in this case is*_ $H_0: \beta = 0$. _*But, the large t-statistc and the small p-value (<2e-16) allows us to reject the null hypothesis. Therefore, there is a significant relationship between x and y.*_

$~$

(b) Now perform a simple linear regression of x onto y without an intercept, and report the coefficient estimate, its standard error, and the corresponding t-statistic and p-values associated with the null hypothesis H0 : ?? = 0. Comment on these results.

```{r}
fit2 <- lm(x~y + 0)
summary(fit2)
```
_*The coefficient estimate*_ $\hat\beta$ _*is 0.39111, the standard error of*_ $\hat\beta$ _*is 0.02089, the t-statistic is 18.73, and the p-value is <2e-16. The null hypothesis in this case is*_ $H_0: \beta = 0$. _*But, the large t-statistc and the small p-value (<2e-16) allows us to reject the null hypothesis. Therefore, there is a significant relationship between y and x.*_

$~$

(c) What is the relationship between the results obtained in (a) and (b)?
_*In both (a) and (b), the values for the t-statistic and the p-values are the same. This implies that both of them reflect the same line, i.e.,*_ $y = 2x + \epsilon$ _*can also be written as*_ $x = 0.5(y - \epsilon)$.

$~$

(d) For the regression of Y onto X without an intercept, the t-statistic for H0 : ?? = 0 takes the form ^??/SE( ^ ??), where ^ ?? is given by (3.38), and where

$$SE(\hat\beta) = \sqrt{\frac{\sum_{i = 1}^n{(y_i - x_i\hat\beta)^2}}{(n-1)\sum_{i^{'} = 1}^nx_{i^{'}}^2}}$$
(These formulas are slightly different from those given in Sections 3.1.1 and 3.1.2, since here we are performing regression without an intercept.) Show algebraically, and confirm numerically in R, that the t-statistic can be written as:
$$\frac{(\sqrt{n-1})\sum_{i = 1}^nx_iy_i}{\sqrt{(\sum_{i=1}^n{x_{i}}^2)(\sum_{i^{'}=1}^n{y_{i^{'}}}^2) - (\sum_{i^{'}=1}^nx_{i^{'}}y_{i^{'}})^2}}$$
_*We know*_ 
$$t = \frac{\hat\beta}{SE(\hat\beta)} = \frac{\frac{\sum_{i = 1}^{n}x_iy_i}{\sum_{i^{'} = 1}^{n}x_{i^{'}}^2}}{\sqrt{\frac{\sum_{i = 1}^n{(y_i - x_i\hat\beta)^2}}{(n-1)\sum_{i^{'} = 1}^nx_{i^{'}}^2}}} = \frac{(\sqrt{n-1})\sum_{i = 1}^nx_iy_i}{\sqrt{(\sum_{i=1}^n{x_{i}}^2)(\sum_{i=1}^n(y_i^2 - 2y_ix_i\hat\beta + x_i^2{\hat\beta}^2))}} = \frac{(\sqrt{n-1})\sum_{i = 1}^nx_iy_i}{\sqrt{\sum_{i = 1}^nx_i^2\sum_{i = 1}^ny_i^2 - \sum_{i = 1}^nx_i^2\beta(2\sum_{i = 1}^nx_iy_i - \beta\sum_{i = 1}^nx_i^2)}}$$
$$t = \frac{(\sqrt{n-1})\sum_{i = 1}^nx_iy_i}{\sqrt{\sum_{i = 1}^nx_i^2\sum_{i = 1}^ny_i^2 - \sum_{i=1}^nx_iy_i(2\sum_{i=1}^nx_iy_i - \sum_{i=1}^nx_iy_i)}} = \frac{(\sqrt{n-1})\sum_{i = 1}^nx_iy_i}{\sqrt{\sum_{i = 1}^nx_i^2\sum_{i = 1}^ny_i^2 - (\sum_{i=1}^nx_iy_i)^2}}$$
```{r}
n <- length(x)
t <- sqrt(n - 1)*(x %*% y)/sqrt(sum(x^2) * sum(y^2) - (x %*% y)^2)
as.numeric(t)
```
_*The t-statistic above is the same as the one in part (a) and (b).*_

$~$

(e) Using the results from (d), argue that the t-statistic for the regression of y onto x is the same as the t-statistic for the regression of x onto y.

_*If we replace*_ $x_i$ _*with*_ $y_i$ _*in the above equations, we would get the same result.*_


(f) In R, show that when regression is performed with an intercept, the t-statistic for H0 : ??1 = 0 is the same for the regression of y onto x as it is for the regression of x onto y.

```{r}
fit3 <- lm(y ~ x)
summary(fit3)

fit4 <- lm(x ~ y)
summary(fit4)
```
_*As we can see from the above two regressions, the t-statistic for*_ $\beta_1$ _*for both the regressions is the same.*_

$~$

12. This problem involves simple linear regression without an intercept.
(a) Recall that the coefficient estimate ^ ?? for the linear regression of Y onto X without an intercept is given by (3.38). Under what circumstance is the coefficient estimate for the regression of X onto Y the same as the coefficient estimate for the regression of Y onto X?

_*The coefficient estimate for the regression of Y onto X is given by*_ $$\hat\beta = \frac{\sum_{i=1}^nx_iy_i}{\sum_{i^{'}=1}^nx_{i^{'}}^2}$$ 
_*The coefficient estimate for the regression of X onto Y is given by*_ $$\hat\beta = \frac{\sum_{i=1}^nx_iy_i}{\sum_{i^{'}=1}^ny_{i^{'}}^2}$$

_*Therefore, the coefficient estimate for the regression of X onto Y is the same as the coefficient estimate of Y onto X when*_ $\sum_{i^{'}=1}^nx_{i^{'}}^2 = \sum_{i^{'}=1}^ny_{i^{'}}^2$

$~$

(b) Generate an example in R with n = 100 observations in which the coefficient estimate for the regression of X onto Y is different from the coefficient estimate for the regression of Y onto X.

```{r}
x <- 1:100
sum(x^2)

y <- 2*x+rnorm (100)
sum(y^2)

lm1 <- lm(y ~ x)
summary(lm1)

lm2 <- lm(x ~ y)
summary(lm2)
```

$~$

(c) Generate an example in R with n = 100 observations in which the coefficient estimate for the regression of X onto Y is the same as the coefficient estimate for the regression of Y onto X.

```{r, warning=FALSE, message=FALSE}
x <- 1:100
sum(x^2)

y <- 100:1
sum(y^2)

lm1 <- lm(y ~ x)
summary(lm1)

lm2 <- lm(x ~ y)
summary(lm2)
```

$~$

13. In this exercise you will create some simulated data and will fit simple linear regression models to it. Make sure to use set.seed(1) prior to starting part (a) to ensure consistent results.
(a) Using the rnorm() function, create a vector, x, containing 100 observations drawn from a N(0, 1) distribution. This represents a feature, X.

```{r}
set.seed(100)
x <- rnorm(100, mean = 0, sd = 1)
```

$~$

(b) Using the rnorm() function, create a vector, eps, containing 100 observations drawn from a N(0, 0.25) distribution i.e. a normal distribution with mean zero and variance 0.25.

```{r}
eps <- rnorm(100, mean = 0, sd = 0.25)
```

$~$

(c) Using x and eps, generate a vector y according to the model Y = ???1 + 0.5X + $\epsilon$. (3.39) What is the length of the vector y? What are the values of ??0 and ??1 in this linear model?

```{r}
y <- -1 +0.5*x + eps
length(y)
```
_*The values for the coefficient estimates are:*_ $\beta_0 = -1$ _*and*_ $\beta_1 = 0.5$.

$~$

(d) Create a scatterplot displaying the relationship between x and y. Comment on what you observe.

```{r, fig.align='center', fig.width=4, fig.height=3}
plot(x,y)
```
_*From the scatterplot, x and y have a linear relationship.*_

$~$

(e) Fit a least squares linear model to predict y using x. Comment on the model obtained. How do ^ ??0 and ^ ??1 compare to ??0 and ??1?

```{r}
fit5 <- lm(y ~ x)
summary(fit5)
```
_*From the above summary,*_ $\hat\beta_0 and \hat\beta_1$ _*are close to the values for*_ $\beta_0 and \beta_1$.

$~$

(f) Display the least squares line on the scatterplot obtained in (d). Draw the population regression line on the plot, in a different color. Use the legend() command to create an appropriate legend.

```{r}
plot(x,y)
abline(fit5, col = "red", lwd = 2)
abline(-1, 0.5, col = "green", lwd = 2)
legend("bottomright", c("Least Squares", "Population Regression"), col = c("red", "green"), lty = c(1,1))
```

$~$

(g) Now fit a polynomial regression model that predicts y using x and x2. Is there evidence that the quadratic term improves the model fit? Explain your answer.

```{r}
fit6 <- lm(y ~ x + I(x^2))
summary(fit6)
```
_*The inclusion of a quadratic term does not improve the model much. The adjusted R-squared changed from 0.8559 to 0.857, which is a very small improvement. The RSE also shows little improvement from 0.1982 to 0.1974. This can be because, as seen previously, x and y share a linear relationship.*_

$~$

(h) Repeat (a)-(f) after modifying the data generation process in such a way that there is less noise in the data. The model (3.39) should remain the same. You can do this by decreasing the variance of the normal distribution used to generate the error term $\epsilon$ in (b). Describe your results.

```{r, fig.align='center', fig.width=6, fig.height=4, warning=FALSE, message=FALSE}
set.seed(100)
eps <- rnorm(100, sd = 0.01)
y <- -1 + 0.5*x + eps
plot(x,y)
fit7 <- lm(y ~ x)
summary(fit7)
```
_*The t-statistic and the p-value both show that the coefficient estimate of x is significant. As we reduced the noise, the R-squared and RSE values imply a perfect fit, a perfect linear relationship.*_

$~$

(i) Repeat (a)-(f) after modifying the data generation process in such a way that there is more noise in the data. The model (3.39) should remain the same. You can do this by increasing the variance of the normal distribution used to generate the error term $\epsilon$ in (b). Describe your results.

```{r}
set.seed(100)
eps <- rnorm(100, sd = 0.5)
x <- rnorm(100)
y <- -1 + 0.5*x + eps
fit8 <- lm(y ~ x)
summary(fit8)
plot(x,y)
abline(fit8, col = "red")
abline(-1, 0.5, col = "blue")
legend("bottomright", c("Least Squares", "Population Regression"), col = c("red", "blue"), lty = c(1,1))
```
_*Increasing the variance of the normal distribution led to an increase in the RSE value and a drastic decrease in the R-Squared value. The two regression lines are still quite close given the large dataset we have.*_

$~$

(j) What are the confidence intervals for ??0 and ??1 based on the original data set, the noisier data set, and the less noisy data set? Comment on your results.

```{r}
original <- c(confint(fit5))
print(paste0("Confidence interval for ??0 in the original data set: ", "[",original[1],",", original[3],"]"))
print(paste0("Confidence interval for ??1 in the original data set: ", "[",original[2],",", original[4],"]"))
```


```{r}
noisier <- c(confint(fit8))
print(paste0("Confidence interval for ??0 in the noisier data set: ", "[",noisier[1],",", noisier[3],"]"))
print(paste0("Confidence interval for ??1 in the noisier data set: ", "[",noisier[2],",", noisier[4],"]"))
```


```{r}
less_noisy <- c(confint(fit7))
print(paste0("Confidence interval for ??0 in the less noisy data set: ", "[",less_noisy[1],",", less_noisy[3],"]"))
print(paste0("Confidence interval for ??1 in the less noisy data set: ", "[",less_noisy[2],",", less_noisy[4],"]"))
```

_*The intervals seem to be centered around 0.5. With more noise, the confidence intervals become wider and with lesser noise, narrower. The confidence intervals for the less noisy data set are as seen because the model is a perfect fit for the true linear relationship between x and y. Also, the R-squared value = 1 and the extremely small RSE suggest that the model is a perfect fit and that the coefficient estimates are almost equal to the true parameter values.*_

$~$

14. This problem focuses on the collinearity problem.
(a) Perform the following commands in R:

```{r}
set.seed (1)
x1=runif (100)
x2 =0.5* x1+rnorm (100) /10
y=2+2* x1 +0.3* x2+rnorm (100)
```

The last line corresponds to creating a linear model in which y is a function of x1 and x2. Write out the form of the linear model. What are the regression coefficients?

_*The linear model is of the form:*_ $Y = 2 + 2X_1 + 0.3X_2 + \epsilon$, _*where*_ $\epsilon$ _*is a N(0,1) random variable. The regression coefficients are*_ $\beta_0 = 2, \beta_1 = 2 and \beta_2 = 0.3$.

$~$

(b) What is the correlation between x1 and x2? Create a scatterplot displaying the relationship between the variables.

```{r}
print(paste0("Correlation between x1 and x2: ",cor(x1, x2)))

plot(x1, x2)
```
_*x1 and x2 seem to be highly correlated.*_


(c) Using this data, fit a least squares regression to predict y using x1 and x2. Describe the results obtained. What are ^ ??0, ^ ??1, and ^ ??2? How do these relate to the true ??0, ??1, and ??2? Can you reject the null hypothesis H0 : ??1 = 0? How about the null hypothesis H0 : ??2 = 0?

```{r}
fit9 <- lm(y ~ x1 + x2)
summary(fit9)
```
_*The coefficient estimate for ??0 is significant. Though the estimate for ??1 is not completely two standard errors away from the mean, the corresponding p-value is less than 0.05 indicating that the coefficient is significant, and hence, we can the null hypothesis that ??1 = 0. As for the estimate for ??1, the p-value, much greater than 0.05, suggests that the coefficient is not statistically significant.*_

$~$

(d) Now fit a least squares regression to predict y using only x1. Comment on your results. Can you reject the null hypothesis H0 : ??1 = 0?

```{r}
fit10 <- lm(y ~ x1)
summary(fit10)
```
_*The t-statistic for the estimate ??1 is more than 2 and the p-value is much loer than 0.05. Hence, we can reject the null hypothesis H0 : ??1 = 0. The coefficient is statistically significant.*_

$~$

(e) Now fit a least squares regression to predict y using only x2. Comment on your results. Can you reject the null hypothesis H0 : ??1 = 0?

```{r}
fit11 <- lm(y ~ x2)
summary(fit11)
```
_*The t-statistic for the estimate ??1 is more than 2 and the p-value is much loer than 0.05. Hence, we can reject the null hypothesis H0 : ??1 = 0. The coefficient is statistically significant.*_

$~$

(f) Do the results obtained in (c)-(e) contradict each other? Explain your answer.

_*The results are not contradictory because the model in (c) has the effect of x1 and x2 together and the models in (d) and (e) see the effects of x1 and x2 on y individually. Since there is a correlation between x1 and x2, the standard error of the coefficient estimate for ??1 becomes larger than it should be when both x1 and x2 are included in the model. Also, the importance of x2 for y in the (c) model may have been masked due to the presence of correlation.*_

$~$

(g) Now suppose we obtain one additional observation, which was unfortunately mismeasured.

```{r}
x1=c(x1 , 0.1)
x2=c(x2 , 0.8)
y=c(y,6)
```

Re-fit the linear models from (c) to (e) using this new data. What effect does this new observation have on the each of the models? In each model, is this observation an outlier? A high-leverage point? Both? Explain your answers.

```{r, fig.width=3, fig.height=3, fig.show='hold'}
fit12 <- lm(y ~ x1 + x2)
summary(fit12)
plot(fit12)

fit13 <- lm(y ~ x1)
summary(fit13)
plot(fit13)

fit14 <- lm(y ~ x2)
summary(fit14)
plot(fit14)
```

_*In the model with both x1 and x2 as predictors, the last point seems to be a high leverage point, from the residuals vs leverage plot. In the model with x1 as the sole predictor, the last point is an outlier. In the model with x2 as the sole predictor, the last point is a high leverage point.*_