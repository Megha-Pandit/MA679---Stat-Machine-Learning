---
title: "03 Classification Homework"
author: "Megha Pandit"
date: "February 5, 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(caret)
library(mlr)
library(MASS)
library(ISLR)
library(class)
```

6. Suppose we collect data for a group of students in a statistics class with variables X1 =hours studied, X2 =undergrad GPA, and Y = receive an A. We fit a logistic regression and produce estimated coefficient, $\hat\beta_0 = -6$, $\hat\beta_1 = 0.05$, $\hat\beta_2= 1$.
(a) Estimate the probability that a student who studies for 40 h and has an undergrad GPA of 3.5 gets an A in the class.

The probability that a student who studies for 40h and has an undergrad GPA of 3.5 gets an A is given by:

$$P(Y=1) = \frac{e^{-6 + 0.05*40 + 1*3.5}}{1 + e^{-6 + 0.05*40 + 1*3.5}} = \frac{e^{-0.5}}{1 + e^{-0.5}} = \frac{0.6065}{1.6065} = 0.3775$$
The probability of the student getting an A is 37.75%.

$~$

(b) How many hours would the student in part (a) need to study to have a 50% chance of getting an A in the class?

We could write the logistic regression equation in terms of log odds as below:
$$log(\frac{P(Y = 1)}{1 - P(Y = 1)}) = \hat\beta_0 + \hat\beta_1X_1 + \hat\beta_2X_2$$
$$log(\frac{0.5}{1 - 0.5}) = -6 + 0.05X_1 + 1*3.5$$
$$-6 + 0.05X_1 + 3.5 = 0$$
$$X_1 = \frac{2.5}{0.05} = 50$$
Therefore, the student would need to study for 50hrs to have a 50% chance of getting an A.

$~$

8. Suppose that we take a data set, divide it into equally-sized training and test sets, and then try out two different classification procedures. First we use logistic regression and get an error rate of 20% on the training data and 30% on the test data. Next we use 1-nearest neighbors (i.e. K = 1) and get an average error rate (averaged over both test and training data sets) of 18%. Based on these results, which method should we prefer to use for classification of new observations? Why?

_*Since the KNN is performed with K=1, it will yield a training error rate of zero while also overfitting the data. Since the error rate averaged over both training and test data is 18%, the test error rate must have been 36%. This is clearly higher than the test error rate that logistic regression produces. Therefore, I would choose logistic regression to classify these observations.*_

$~$

9. This problem has to do with odds.
(a) On average, what fraction of people with an odds of 0.37 of defaulting on their credit card payment will in fact default?

The fraction who will default is:
$$\frac{0.37}{1 + 0.37} = \frac{0.37}{1.37} = 0.27$$
or 27%.

$~$

(b) Suppose that an individual has a 16% chance of defaulting on her credit card payment. What are the odds that she will default?

The odds that the individual will default are: 
$$\frac{0.16}{1 - 0.16} = \frac{0.16}{0.84} = 0.19$$
or 19%

$~$

10. This question should be answered using the Weekly data set, which is part of the ISLR package. This data is similar in nature to the Smarket data from this chapter's lab, except that it contains 1, 089 weekly returns for 21 years, from the beginning of 1990 to the end of 2010.
(a) Produce some numerical and graphical summaries of the Weekly data. Do there appear to be any patterns?

```{r, fig.width=3, fig.height=3, fig.show='hold'}
library(ISLR)
data("Weekly") 

summary(Weekly)
ggplot(data = Weekly) +
  aes(y = Volume, x = seq_along(Volume)) +
  geom_point(color = "pink") +
  theme_minimal()+
  ggtitle("Volume of shares traded")

ggplot(Weekly)+
  aes(x = Year, y = Lag1)+
  geom_point(color = "orange")+
  ggtitle("Lag1")

ggplot(data = Weekly) +
  aes(x = Direction, y = Volume, fill = Direction) +
  geom_boxplot() +
  scale_fill_brewer(palette = "Set1") +
  theme_minimal()

ggplot(data = Weekly) +
  aes(x = Today, y = Lag1) +
  geom_point(color = "#e7298a") +
  theme_minimal()
```
_*The mean values of all the Lags are similar and their minimum and maximum values are the same, although they are percentage returns for different lengths of time. The volume of shares traded has been increasing over the years. There is no relationship between any of the lags and today's returns.*_

$~$

(b) Use the full data set to perform a logistic regression with Direction as the response and the five lag variables plus Volume as predictors. Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?

```{r}
fit <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly,
           family = binomial(link = logit))
summary(fit)
```
_*Lag2 appears to be the only statistically significant predictor. All the others have small t-statistics and large p-values.*_

$~$

(c) Compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.

```{r}
pred <- predict(fit, type = "response")
pred_glm <- rep("Down", length(pred))
pred_glm[pred > 0.5] <- "Up"

confusionMatrix(data = as.factor(pred_glm), reference = Weekly$Direction)
```
_*Overall, the model has an accuracy of 56.11%. "Down" is considered the positive class in this case. The positive predictive value is 0.529. This implies that 52.9% out of all the "Down" predictions made by the model are actually "Down". And 56.4% of all the "Up" predictions made by the model are actually "Up". Regarding the mistakes made by the model, the False Positive Rate or the percentage of "Down" predictions which are actually "Up" is 0.4705 or 47.05%. The False Negative Rate or the percentage of "Up" predictions which are actually "Down" is 43.56%.*_

$~$

(d) Now fit the logistic regression model using a training data period from 1990 to 2008, with Lag2 as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010).

```{r}
train <- Weekly[Weekly$Year < 2009,]
test <- Weekly[Weekly$Year > 2008,]
fit1 <- glm(Direction ~ Lag2, data = train, family = binomial)
summary(fit1)

pred1 <- predict(fit1, newdata = test, type = "response")
pred1_glm <- rep("Down", length(pred1))
pred1_glm[pred1 > 0.5] <- "Up"
confusionMatrix(as.factor(pred1_glm), reference = test$Direction)
```
_*The overall fraction of correct predictions for the held out data is (9 + 56)/(9 + 5 + 34 + 56) = 65/104 = 0.625, which is stated as the accuracy of the model, in the summary for the confusion matrix.*_

$~$

(e) Repeat (d) using LDA.

```{r}
lda1 <- lda(Direction ~ Lag2, data = train)
lda1

pred.lda <- predict(lda1, newdata = test)
confusionMatrix(pred.lda$class, reference = test$Direction)
```
_*The overall fraction of correct predictions through LDA is also 0.625 or 62.5%.*_

$~$

(f) Repeat (d) using QDA.

```{r}
qda1 <- qda(Direction ~ Lag2, data = train)
qda1

pred.qda <- predict(qda1, newdata = test)
confusionMatrix(as.factor(pred.qda$class), reference = test$Direction)
```
_*The overall fraction of correct predictions in the case od QDA is 0.5865 or 58.65%, which is lower than that for LDA and Logistic Regression.*_

$~$

(g) Repeat (d) using KNN with K = 1.

```{r}
train.x <- cbind(train[,3])
test.x <- cbind(test[,3])
train.direction <- train[,9]
test.direction <- test[,9]

set.seed(1)
knn1 <- knn(train.x, test.x, train.direction, k = 1)
table(knn1, test$Direction)
```
_*The overall fraction of correct predictions in the case of KNN with k = 1, is 52/104 = 50%.*_

$~$

(h) Which of these methods appears to provide the best results on this data?

_*Logistic regression and LDA, both give the best results among all the approaxches. The accuracy is 62.5%.*_

$~$

(i) Experiment with different combinations of predictors, including possible transformations and interactions, for each of the methods. Report the variables, method, and associated confusion matrix that appears to provide the best results on the held out data. Note that you should also experiment with values for K in the KNN classifier.

_*Out of all the models with different combinations of predictors and interactions, the one that has only Lag2 as a predictor gives the best results for logistic regression. Therefore, we proceed to try different combinations of predictors for the other methods: LDA, QDA and KNN.*_
```{r}
train <- Weekly[Weekly$Year < 2009,]
test <- Weekly[Weekly$Year > 2008,]

lda2 <- lda(Direction ~ Lag1 + Lag2 + Lag2*Lag1, data = train)
lda2

pred.lda2 <- predict(lda2, newdata = test)
confusionMatrix(pred.lda2$class, reference = test$Direction)
```
_*The LDA model above gives an accuracy of 57.7%. Similar to logistic regression results, LDA results for the model with just Lag2 as a predictor were much better than with any other combinations of predictors and even with interactions.*_

```{r}
qda2 <- qda(Direction ~ Lag2 + I(Lag2^2), data = train)
qda2

pred.qda2 <- predict(qda2, newdata = test)
confusionMatrix(as.factor(pred.qda2$class), reference = test$Direction)
```
_*Adding a quadratic term Lag2^2 improved the QDA result accuracy from 58.65% to 62.5%.*_

```{r}
train.x <- cbind(train[,3])
test.x <- cbind(test[,3])
train.direction <- train[,9]
test.direction <- test[,9]
  
set.seed(1)
knn2 <- knn(train.x, test.x, train.direction, k = 5)
table(knn2, test$Direction)

mean(knn2 == test.direction)
```
_*k = 5 improves the accuarcy of the KNN approach from 50% to 53.84%.*_

```{r}
set.seed(1)
knn4 <- knn(train.x, test.x, train.direction, k = 10)
table(knn4, test$Direction)

mean(knn4 == test.direction)
```
_*k = 10 has a marginal improvement over k = 5.*_

```{r}
set.seed(1)
knn5 <- knn(train.x, test.x, train.direction, k = 20)
table(knn5, test$Direction)

mean(knn5 == test.direction)
```
_*Larger values of k, i.e., around k = 20, seem to produce better results for the KNN approach with only Lag2 as a predictor.*_

$~$

11.In this problem, you will develop a model to predict whether a given car gets high or low gas mileage based on the Auto data set.
(a) Create a binary variable, mpg01, that contains a 1 if mpg contains a value above its median, and a 0 if mpg contains a value below its median. You can compute the median using the median() function. Note you may find it helpful to use the data.frame() function to create a single data set containing both mpg01 and the other Auto variables.

```{r}
data("Auto")
Auto$mpg01 <- c()
for (i in 1:length(Auto$mpg)) {
  
  if (Auto$mpg[i] > median(Auto$mpg)){
   Auto$mpg01[i] <- 1
}
  else {
    Auto$mpg01[i] <- 0
  }
}

```

$~$

(b) Explore the data graphically in order to investigate the association between mpg01 and the other features. Which of the other features seem most likely to be useful in predicting mpg01? Scatterplots and boxplots may be useful tools to answer this question. Describe your findings.

```{r, fig.width=4, fig.height=3, fig.show='hold'}

ggplot(data = Auto) +
  aes(x = displacement, y = weight) +
  geom_point(aes(color = factor(Auto$mpg01))) +
  theme_minimal()+
  labs(color = "mpg01")+
  ggtitle("displacement vs weight")

ggplot(data = Auto) +
  aes(x = displacement, y = horsepower) +
  geom_point(aes(color = factor(Auto$mpg01))) +
  theme_minimal()+
  labs(color = "mpg01")+
  ggtitle("displacement vs horsepower")

ggplot(data = Auto) +
  aes(x = displacement, y = acceleration) +
  geom_point(aes(color = factor(Auto$mpg01))) +
  theme_minimal()+
  labs(color = "mpg01")+
  ggtitle("displacement vs acceleration")
```

_*From the displacement vs weight plot, vehicles with mpg above the median mpg tend to have lower displacement and lower weight, and those with mpg lower than the median mpg have higher displacement and higher weight.*_

_*From the displacement vs horsepower plot, it is seen that vehicles with higher than median mpg have lower displacement and horsepower.*_

_*From the displacement vs acceleration, vehicles with higher than median mpg have higher acceleration and lower displacement. And vehicles with mpg lower than the median mpg have lower acceleration and higher displacement.*_

```{r}
par(mfrow = c(3,2))
boxplot(acceleration~mpg01, data = Auto, main = "Acceleration ~ mpg01")
boxplot(horsepower~mpg01, data = Auto, main = "Horsepower ~ mpg01")
boxplot(weight~mpg01, data = Auto, main = "Weight ~ mpg01")
boxplot(displacement~mpg01, data = Auto, main = "Displacement ~ mpg01")
boxplot(cylinders~mpg01, data = Auto, main = "Cylinders ~ mpg01")
```
_*From the above plots, cylinders, displacement, weight, and horsepower vary with respect to mpg01.*_

$~$

(c) Split the data into a training set and a test set.

```{r}
data.train <- Auto[Auto$year < 80,]
data.test <- Auto[Auto$year > 79,]
```

$~$

(d) Perform LDA on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained?

```{r}
lda3 <- lda(mpg01 ~ cylinders + displacement + weight, data = data.train)
lda3

pred.lda3 <- predict(lda3, newdata = data.test)
confusionMatrix(pred.lda3$class, factor(data.test$mpg01))

mean(pred.lda3$class != data.test$mpg01)
```
_*The test error rate of the model is 11.76%.*_
 
$~$

(e) Perform QDA on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained?

```{r}
qda3 <- qda(mpg01 ~ cylinders + displacement + weight, data = data.train)
qda3

pred.qda3 <- predict(qda3, newdata = data.test)
confusionMatrix(pred.qda3$class, factor(data.test$mpg01))

mean(pred.qda3$class != data.test$mpg01)
```
_*The test error rate of the QDA model is 12.94%.*_

$~$

(f) Perform logistic regression on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained?

```{r}
fit3 <- glm(mpg01 ~ cylinders + displacement + weight + horsepower, data = data.train, family = binomial)
summary(fit3)

pred3 <- predict(fit3, newdata = data.test, type = "response")
pred3.glm <- rep(0, length(pred3))
pred3.glm[pred3 > 0.5] <- 1
confusionMatrix(as.factor(pred3.glm), as.factor(data.test$mpg01))
```
_*Including horsepower as a predictor in logistic regression drastically improves the accuracy of the model. The test error rate of this model is 17.65%.*_

$~$

(g) Perform KNN on the training data, with several values of K, in order to predict mpg01. Use only the variables that seemed most associated with mpg01 in (b). What test errors do you obtain? Which value of K seems to perform the best on this data set?

```{r}
train.auto <- cbind(data.train[,c(2,3,4,5)])
test.auto <- cbind(data.test[,c(2,3,4,5)])
train.mpg <- data.train[,10]
test.mpg <- data.test[,10]
  
set.seed(1)
knn6 <- knn(train.auto, test.auto, train.mpg, k = 5)
table(knn6, data.test$mpg01)

mean(knn6 != test.mpg)
```
_*k = 5 yields a test error rate of 22.35%.*_

```{r}
set.seed(1)
knn7 <- knn(train.auto, test.auto, train.mpg, k = 10)
table(knn7, data.test$mpg01)

mean(knn7 != test.mpg)
```
_*Surprisingly, k = 10 does not show any improvement over k = 5.*_

```{r}
set.seed(1)
knn8 <- knn(train.auto, test.auto, train.mpg, k = 20)
table(knn8, data.test$mpg01)

mean(knn8 != test.mpg)
```
_*k = 20 yields a better result with the test error rate at 18.8%.*_

$~$

12. This problem involves writing functions.
(a) Write a function, Power(), that prints out the result of raising 2 to the 3rd power. In other words, your function should compute $2^3$ and print out the results.
Hint: Recall that x^a raises x to the power a. Use the print() function to output the result.

```{r}
Power <- function(){
  2^3
}
Power()
```

$~$

(b) Create a new function, Power2(), that allows you to pass any two numbers, x and a, and prints out the value of x^a. You can do this by beginning your function with the line:
This should output the value of $3^8$, namely, 6,561.

```{r}
Power2 <- function (x,a){
  x^a
}
Power2 (3,8)
```

$~$

(c) Using the Power2() function that you just wrote, compute $10^3, 8^{17}, and 131^3.$

```{r}
Power2 (10,3)
Power2 (8,17)
Power2 (131,3)
```

$~$

(d) Now create a new function, Power3(), that actually returns the result x^a as an R object, rather than simply then you can simply return() this return() result, using the following line:

```{r}
Power3 <- function(x,a){
  result <- x^a
  return(result)
}
```

$~$

(e) Now using the Power3() function, create a plot of $f(x) = x^2$. The x-axis should display a range of integers from 1 to 10, and the y-axis should display $x^2$. Label the axes appropriately, and use an appropriate title for the figure. Consider displaying either the x-axis, the y-axis, or both on the log-scale. You can do this by using log=''x'', log=''y'', or log=''xy'' as arguments to the plot() function.

```{r}
x <- 1:10
y <- sapply(x, function(x)Power3(x,2))
plot(x, y, log = "x", type = "l", main = "x^2 vs x")
```

$~$

(f) Create a function, PlotPower(), that allows you to create a plot of x against x^a for a fixed a and for a range of values of x. For instance, if you call
PlotPower (1:10 ,3)
then a plot should be created with an x-axis taking on values 1, 2, . . . , 10, and a y-axis taking on values 13, 23, . . . , 103.

```{r}
PlotPower <- function(x,a){
  y <- sapply(x, function(x){x^a})
  plot(x,y, type = "l", main = paste('x~x^',a,sep = ''))
}
PlotPower(1:10,3)
```

$~$

13. Using the Boston data set, fit classification models in order to predict whether a given suburb has a crime rate above or below the median. Explore logistic regression, LDA, and KNN models using various subsets of the predictors. Describe your findings.

```{r}
data("Boston")
mcrim <- ifelse(Boston$crim > median(Boston$crim), 1, 0)
df <- data.frame(Boston, mcrim)

par(mfrow = c(2,2))
boxplot(medv~mcrim, df, main='medv ~ mcrim')
boxplot(lstat~mcrim, df, main='lstat ~ mcrim')
boxplot(dis~mcrim, df, main='dis ~ mcrim')
boxplot(age~mcrim, df, main='age ~ mcrim')
```

```{r}
train.df <- df[1:356,]
test.df <- df[357:506,]

fit4 <- glm(mcrim ~ dis + age + lstat, data = train.df, family = binomial)
summary(fit4)

pred4 <- predict(fit4, newdata = test.df, type = "response")
pred4.glm <- rep(0, length(pred4))
pred4.glm[pred4 > 0.5] <- 1
confusionMatrix(as.factor(pred4.glm), as.factor(test.df$mcrim))
```
_*Logistic Regression gives an accuracy of 82.67%. Adding medv as a predictor does not change the accuracy. The test error rate is 17.33%.*_

```{r}
lda4 <- lda(mcrim ~ dis + age + lstat + medv, data = train.df)
lda4

pred4.lda <- predict(lda4, newdata = test.df)
confusionMatrix(pred4.lda$class, as.factor(test.df$mcrim))
```
_*Adding medv as a predictor in LDA improves the model slightly, giving an accuracy of 84%. The test error rate is 16%.*_

```{r}
qda4 <- qda(mcrim ~ dis + age + lstat, data = train.df)
qda4

pred4.qda <- predict(qda4, newdata = test.df)
confusionMatrix(pred4.qda$class, as.factor(test.df$mcrim))
```
_*Including medv as a predictor in QDA gives an accuracy of around 82% whereas excluding it gives an accuracy of 84%. The test error rate for this model is 16%.*_

```{r}
train.boston <- cbind(train.df[,c(7,8,13,14)])
test.boston <- cbind(test.df[,c(7,8,13,14)])
train.mcrim <- train.df[,15]
test.mcrim <- test.df[,15]
  
set.seed(1)
knn9 <- knn(train.boston, test.boston, train.mcrim, k = 5)
table(knn9, test.df$mcrim)

mean(knn9 != test.mcrim)
```
_*KNN with k = 5 yields a test error rate of 24.67%.*_

```{r}
set.seed(1)
knn10 <- knn(train.boston, test.boston, train.mcrim, k = 10)
table(knn10, test.df$mcrim)

mean(knn10 != test.mcrim)
```
_*k = 10 yields a test error rate of 28%, higher than that for k = 5.*_

```{r}
set.seed(1)
knn11 <- knn(train.boston, test.boston, train.mcrim, k = 20)
table(knn11, test.df$mcrim)

mean(knn11 != test.mcrim)
```
_*k = 20 yields an even higher test error rate of 30%.*_

```{r}
set.seed(1)
knn12 <- knn(train.boston, test.boston, train.mcrim, k = 2)
table(knn12, test.df$mcrim)

mean(knn12 != test.mcrim)
```
_*k = 2 also yields a test error rate of 30%. Therefore, the best result could be for k = 5.*_