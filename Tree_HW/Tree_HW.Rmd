---
title: "Tree-Based Methods HW"
author: "Megha Pandit"
date: "March 3, 2019"
output: pdf_document
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(randomForest)
library(ISLR)
library(tree)
library(gbm)
library(knitr)
```

#Problem 1

```{r}
data <- data.frame(c(25,25,75,76,60,85), c(76,30,80,55,25,20))
plot(data, xlim = c(0,100), ylim = c(0,110), xlab = "X1", ylab = "X2", xaxt = "n", yaxt = "n", pch = "")
lines(x = c(50,50), y = c(0,100))
lines(x = c(0,50), y = c(70,70))
lines(x = c(50,100), y = c(65,65))
lines(x = c(50,100), y = c(45,45))
lines(x = c(75,75), y = c(0,45))

text(data, labels = paste("R", 1:6, sep = ""))

text(x = 50, y = 108, labels = c("t1"), col = "orange")
text(x = -0.5, y = 70, labels = c("t2"), col = "orange")
text(x = 102, y = 65, labels = c("t3"), col = "orange")
text(x = 102, y = 45, labels = c("t4"), col = "orange")
text(x = 75, y = 0, labels = c("t5"), col = "orange")

```

$~$
$~$

#Problem 2

```{r, out.width= "85%"}
include_graphics("C:/Users/GP/Desktop/MEGHA/SemII/MA679 - Appl Stat Learning/Homework/MA679---Stat-Machine-Learning/Tree_HW/Image.jpeg")
```


$~$
$~$

#Problem 3

```{r}
p <- seq(0,1,0.01)

#For two classes
gini <- 2*p*(1-p)
error <- 1 - pmax(p, 1-p)
entropy <- -(p*log(p) + (1-p)*log(1-p))

plot(NA, xlim = c(0,1), ylim = c(0,1), xlab = "p", ylab = "f")
lines(p, gini, type = "l", col = "red", lwd = 1.5)
lines(p, error, type = "l", col = "black", lwd = 1.5)
lines(p, entropy, type = "l", col = "orange", lwd = 1.5)

legend(x = "topright", legend = c("Gini Index", "Class Error", "Cross Entropy"),
       col = c("red", "black", "orange"), lty = 1)

```

$~$
$~$


#Problem 5
P(Class is Red | X) is greater than 0.5 in 6 of the 10 times. Therefore, according to the majority vote way, the final classification is Red. 
According to the approach based on average probability, the average probability for the 10 estimates is 0.45, i.e., P(Class is Red | X) < 0.5, implying that the final classification is Green. 

$~$
$~$

#Problem 7

```{r}
data("Boston")
set.seed(9)
train <- sample(1:nrow(Boston), nrow(Boston)/2)
Boston_train <- Boston[train, -14]
Boston_test <- Boston[-train, -14]
y_train <- Boston[train, 14]
y_test <- Boston[-train, 14]

rf1 <- randomForest(Boston_train, y = y_train, xtest = Boston_test, ytest = y_test, mtry = ncol(Boston) - 1, ntree = 500)
rf2 <- randomForest(Boston_train, y = y_train, xtest = Boston_test, ytest = y_test, mtry = (ncol(Boston) - 1)/2, ntree = 500)
rf3 <- randomForest(Boston_train, y = y_train, xtest = Boston_test, ytest = y_test, mtry = sqrt(ncol(Boston) - 1), ntree = 500)

plot(1:500, rf1$test$mse, type = "l", col = "green", xlab = "Number of Trees", ylab = "Test MSE")
lines(1:500, rf2$test$mse, type = "l", col = "blue")
lines(1:500, rf3$test$mse, type = "l", col = "red")
legend(x = "topright", c("m = p", "m = p/2", "m = sqrt(p)"), col = c("green", "blue", "red"), lty = 1)

```

$~$
$~$

#Problem 8
#(a)
```{r}
data("Carseats")
set.seed(9)
subs <- sample(1:nrow(Carseats), nrow(Carseats)*0.7)
car_train <- Carseats[subs, ]
car_test <- Carseats[-subs, ]

```

#(b)

```{r}
#Regression Tree

rtree <- tree(Sales ~ ., data = car_train)
summary(rtree)

plot(rtree)
text(rtree, cex = 0.65)

#MSE
pred_rtree <- predict(rtree, car_test)
mse_rtree <- mean((car_test$Sales - pred_rtree)^2)
print(paste0("The test MSE for the regression tree is: ", mse_rtree))
```

#(c)

```{r}
#Cross-Validation for tree complexity

cv_rtree <- cv.tree(rtree)
plot(cv_rtree$size, cv_rtree$dev, xlab = "Size of Tree", ylab = "Deviance", type = "b")

#Tree Pruning

prune_rtree <- prune.tree(rtree, best = 6)
plot(prune_rtree)
text(prune_rtree)

#Test MSE for pruned tree

prune_pred <- predict(prune_rtree, car_test)
prune_mse <- mean((prune_pred - car_test$Sales)^2)
print(paste0("The test MSE for the pruned tree is: ", prune_mse))
```

__**The pruned tree gives a slightly lower MSE than the unpruned tree.**__

#(d)

```{r}
#Bagging 

car_bag <- randomForest(Sales ~ ., data = car_train, mtry = 10, importance = TRUE, ntree = 500)
pred_bag <- predict(car_bag, car_test)
bag_mse <- mean((pred_bag - car_test$Sales)^2)

print(paste0("The test MSE for bagging method is: ", bag_mse))

```

__**Bagging reduces the test MSE to 2.936**__

```{r}
#Importance
importance(car_bag)
```

__**Price and ShelveLoc seem to be the two most important variables.**__

#(e)

```{r}
#Random Forest

rf_mse <- c()
for (i in 1:10) {
  car_rf <- randomForest(Sales ~ ., data = car_train, mtry = i, importance = TRUE, ntree = 500)
  pred_rf <- predict(car_rf, car_test)
  rf_mse[i] <- mean((pred_rf - car_test$Sales)^2)
}

#Best model
which.min(rf_mse)

#Minimum MSE
rf_mse[which.min(rf_mse)]

```
__**The best model uses 10 variables at each split. It does not quite reduce the test MSE compared to Bagging.**__

```{r}
importance(car_rf)
```

__**ShelveLoc seems to be the most important variable, followed by Price.**__

$~$
$~$

#Problem 11

#(a)

```{r}
data("Caravan")
Caravan$Purchase <- ifelse(Caravan$Purchase == "No", 0, 1)
crv_train <- Caravan[1:1000, ]
crv_test <- Caravan[1001:5822, ]
```

#(b)

```{r}
#Boosting

set.seed(9)
boost <- gbm(Purchase ~ ., data = crv_train, shrinkage = 0.01, n.trees = 1000, distribution = "bernoulli")
kable(summary(boost), row.names = F)
```

__**PPERSAUT, MKOOPKLA and MOPLHOOG are the three most important variables.**__

#(c)

```{r}
pred_boost <- predict(boost, crv_test, n.trees = 1000, type = "response")
boost_pred <- ifelse(pred_boost > 0.2, 1, 0)
table(crv_test$Purchase, boost_pred)
```

__**The fraction of people who were predicted to make a purchase and who actually made a purchase is 36/(36 + 118), which is 0.2337 or 23.37%.**__

```{r}
#Logistic Regression

crv_glm <- glm(Purchase ~ ., data = crv_train, family = binomial)

pred_glm <- predict(crv_glm, crv_test, type = "response")
glm_pred <- ifelse(pred_glm > 0.2, 1, 0)
table(crv_test$Purchase, glm_pred)
```

__**From Logistic Regression, the fraction of people predicted to make a purchase and who actually made a purchase is 58/(58 + 350), which is 0.1421 or 14.21%. Logistic regression performs worse than Boosting in this scenario.**__

