---
title: "SVM Homework"
author: "Megha Pandit"
date: "March 13, 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(e1071)
library(ISLR)
```

#Problem 3
#(a)

```{r}
x1 <- c(3, 2, 4, 1, 2, 4, 4)
x2 <- c(4, 2, 4, 4, 1, 3, 1)
cols <- c("red", "red", "red", "red", "blue", "blue", "blue")
plot(x1, x2, col = cols, xlim = c(0,5), ylim = c(0,5))
```

#(b)

__**Since the two classes are very distinctly separable, we can see from the plot that the hyperplace must lie in between the points {(2,2), (4,4)} and {(2,1), (4,3)}. Therefore, the hyperplane will pass through the points (2, 1.5) and (4, 3.5). The equation for the line passing through these two points is: **__ $$ x2 = -0.5 + x1$$
__**Therefore, the intercept and slope are -0.5 and 1 respectively.**__

```{r}
#Optimal separating hyperplane
plot(x1, x2, col = cols, xlim = c(0,5), ylim = c(0,5))
abline(-0.5, 1)
```

#(c)

__**The classification rule here would be: **__
$~$
__**Classify as Red if**__ $x2 - x1 +0.5 > 0$ , __**and, classify as Blue if**__ $x2 - x1 +0.5 < 0$

#(d)

```{r}
#Margin for maximal margin hyperplane
plot(x1, x2, col = cols, xlim = c(0,5), ylim = c(0,5))
abline(-0.5, 1)
abline(-1, 1, lty = 2)
abline(0, 1, lty = 2)
```

#(e)

__**The support vectors for the maximal margin classifier are the points (2,1), (2,2), (4,3) and (4,4).**__

#(f)

__**Since the 7th observation is not a support vector, a slight change in its position will not affect the maximal margin hyperplane. This is evident even from the above plot.**__

#(g)

__**The equation**__ $x2 = -0.25 + x1$ __**will also separate all the observations but is not an optimal hyperplane because the margin is smaller than the optimal option.**__

```{r}
plot(x1, x2, col = cols, xlim = c(0,5), ylim = c(0,5))
abline(-0.25, 1)
abline(0, 1, lty = 2)
abline(-0.5, 1, lty = 2)
```

#(h)

```{r}
plot(x1, x2, col = cols, xlim = c(0,5), ylim = c(0,5))
abline(-0.5, 1)
abline(-1, 1, lty = 2)
abline(0, 1, lty = 2)
points(3,1, col = "red")
```

__**From the above plot, we see that after adding the new point, the hyperplane cannot separate the classes.**__

$~$
$~$

#Problem 5
#(a)

```{r}
set.seed(9)
x1 <- runif(500) - 0.5
x2 <- runif(500) - 0.5
y <- 1*(x1^2 - x2^2 > 0)

```

#(b)

```{r}

plot(x1, x2, col = ifelse(y, "red", "blue"))
```

#(c)

```{r}
#Logistic regression
df <- data.frame(x1, x2, y)
fit_glm <- glm(y ~ x1 + x2, data = df, family = binomial)
fit_glm
```

#(d)

```{r}
pred_fit <- predict(fit_glm, data.frame(x1,x2))
plot(x1, x2, col = ifelse(pred_fit > 0, "red", "blue"), pch = ifelse(as.integer(pred_fit > 0) == y, 1,4))
```
__**In the above plot, the circles are the observations that have been classified correctly and the the crosses are the ones that are misclassified. The decision boundary looks linear.**__

#(e)

```{r}
fit_glm1 <- glm(y ~ poly(x1, 2) + poly(x2, 2), data = df, family = binomial)
summary(fit_glm1)

fit_glm2 <- glm(y ~ x1 + x2 + x1*x2, data = df, family = binomial)
summary(fit_glm2)

```

__**None of the coefficient estimates are statistically significant.**__

#(f)

```{r}
pred_fit1 <- predict(fit_glm1, df)
plot(x1, x2, col = ifelse(pred_fit1 > 0, "red", "blue"), pch = ifelse(as.integer(pred_fit1 > 0) == y, 1,4))
```
__**The plot above shows a non-linear decision boundary and all the observations are correctly classified. Also, the decision boundary is similar to the true decision boundary.**__

#(g)

```{r}
#Support Vector Classifier
df$y <- as.factor(df$y)
fit_svc <- svm(y ~ x1 + x2, data = df, kernel = "linear")
pred_svc <- predict(fit_svc, df, type = "response")
plot(x1, x2, col = ifelse(pred_svc != 0, "red", "blue"), pch = ifelse(pred_svc == y, 1,4))
```
__**In the above plot, the circles represent observations that have been classified correctly and crosses represent the observations that have been misclassified.**__

#(h)

```{r}
#SVM with non-linear kernel

fit_svm <- svm(y ~ x1 + x2, data = df, kernel = "polynomial", degree = 2)
pred_svm <- predict(fit_svm, df, type = "response")
plot(x1, x2, col = ifelse(pred_svm != 0, "red", "blue"), pch = ifelse(pred_svm == y, 1,4))
```

__**There is a drastic improvement in classification compared to the linear kernel.**__

#(i)

__**From the results, we see that SVM with a polynomial kernel of degree 2 performs quite well but still misclassifies some observations. In contrast, logistic regression with non-linear functions of predictors (polynomials of degree 2) does not misclassify any observations and definitely performs better than SVM.**__

$~$
$~$

#Problem 7
#(a)

```{r}
data("Auto")
Auto$Y <- ifelse(Auto$mpg > median(Auto$mpg), 1, 0)
Auto$Y <- as.factor(Auto$Y)
```

#(b)

```{r}
set.seed(9)
cost <- data.frame(cost = seq(0.01, 100, length.out = 10))
svm_tune <- tune(svm, Y ~ ., data = Auto, kernel = "linear", ranges = cost)
summary(svm_tune)
plot(svm_tune$performances[,c(1,2)], type = "l")
```

__**Cost = 11.12 seems to perform the best. This is also seen in the plot.**__

#(c)

```{r}
#Polynomial Kernel
para <- data.frame(cost = seq(0.01, 100, length.out = 5), degree = seq(1, 100, length.out = 5))

svm_poly <- tune(svm, Y ~ ., data = Auto, kernel = "polynomial", ranges = para)
summary(svm_poly)
```

__**Cost of 100 with degree 1 seems to perform the best.**__


```{r}
#Radial Kernel

params <- data.frame(cost=seq(0.01,100,length.out = 5),gamma=seq(0.1,100,length.out = 5))
svm_radial <- tune(svm, Y ~ ., data = Auto, kernel = "radial", ranges = params)
summary(svm_radial) 
```

#(d)

```{r, fig.height=3, fig.width=4, fig.show='hold'}
linear <- svm(Y ~ ., data = Auto, kernel = "linear", cost = 11.12)
polynomial <- svm(Y ~ ., data = Auto, kernel = "polynomial", cost = 100, degree = 1)
radial <- svm(Y ~ ., data = Auto, kernel = "radial", cost = 25.0075, gamma = 0.1)
pair_plot <- function(a){
  for (name in names(Auto)[!(names(Auto) %in% c("mpg", "Y", "name"))])
    plot(a, Auto, as.formula(paste("mpg~", name, sep = "")))
}

pair_plot(linear)
```
__**The above are the SVM classification plots for linear kernel.**__

```{r, fig.height=3, fig.width=4, fig.show='hold'}
pair_plot(polynomial)
```

__**Thde above plots are the SVM classification plots for polynomial kernel.**__

```{r, fig.height=3, fig.width=4, fig.show='hold'}
pair_plot(radial)
```

$~$
$~$

#Problem 8
#(a)

```{r}
data("OJ")
set.seed(9)
train_oj <- sample(nrow(OJ), 800)
oj_train <- OJ[train_oj,]
oj_test <- OJ[-train_oj,]
```

#(b)

```{r}
oj_svc <- svm(Purchase ~ ., data = oj_train, kernel = "linear", cost = 0.01)
summary(oj_svc)
```
__**The support vector classifier creates 432 support vectors out of the 800 training observations. Out of the 432 support vectors, 214 belong to level CH and 213 to level MM.**__

#(c)

```{r}
#Training error rate

pred_train <- predict(oj_svc, oj_train)
table(pred_train, oj_train$Purchase)

#Test error rate

pred_test <- predict(oj_svc, oj_test)
table(pred_test, oj_test$Purchase)

(tr_error<- (70+61)/(428+70+61+241))
(te_error <- (33+21)/(143+33+21+73))
```

__**The training error rate is 16.37% and the test error rate is 20%.**__

#(d)

```{r}
#For optimal cost

oj_tune <- tune(svm, Purchase ~ ., data = oj_train, kernel = "linear", ranges =
                  data.frame(cost = seq(0.01, 10, length.out = 25)))
summary(oj_tune)
```
__**From the results, we see that the optimal cost is 3.75625**__

#(e)

```{r}
#Training error rate
oj_svm <- svm(Purchase ~ ., data = oj_train, kernel = "linear", cost = oj_tune$best.parameters$cost)
svm_train <- predict(oj_svm, oj_train)
table(svm_train, oj_train$Purchase)

(tr_err <- (58+64)/(425+58+64+253))

#Test error rate
svm_test <- predict(oj_svm, oj_test)
table(svm_test, oj_test$Purchase)

(te_err <- (27+22)/(142+27+22+79))
```

__**The training error rate is 15.25% and the test error rate is 18.15%.**__

#(f)
#Radial Kernel
```{r}
oj_radial <- svm(Purchase ~ ., data = oj_train, kernel = "radial")
summary(oj_radial)
```

__**The SVM with radial kernel creates 624 support vectors out of the 800 training observations. Out of the 624 support vectors, 313 belong to level CH and 311 to level MM.**__

```{r}
#Training error rate

radial_train <- predict(oj_radial, oj_train)
table(radial_train, oj_train$Purchase)

#Test error rate

radial_test <- predict(oj_radial, oj_test)
table(radial_test, oj_test$Purchase)


```
__**The training error rate is 14.75% and the test error rate is 18.89%.**__

```{r}
#For optimal cost

radial_tune <- tune(svm, Purchase ~ ., data = oj_train, kernel = "radial", ranges =
                  data.frame(cost = seq(0.01, 10, length.out = 25)))
summary(radial_tune)
```

__**The optimal cost is 0.42625.**__

```{r}
#Training error rate
radial_svm <- svm(Purchase ~ ., data = oj_train, kernel = "radial", cost = radial_tune$best.parameters$cost)
svm_rad <- predict(radial_svm, oj_train)
table(svm_rad, oj_train$Purchase)


#Test error rate
svm_rad_test <- predict(radial_svm, oj_test)
table(svm_rad_test, oj_test$Purchase)
```

__**The training error rate is 14.5% and the test error rate is 18.89%.**__

#(g)
#Polynomial Kernel
```{r}
oj_poly <- svm(Purchase ~ ., data = oj_train, kernel = "polynomial", degree = 2)
summary(oj_poly)
```
__**The SVM with polynomial kernel creates 441 support vectors out of the 800 training observations. Out of the 441 support vectors, 224 belong to level CH and 217 to level MM.**__

```{r}
#Training error rate

poly_train <- predict(oj_poly, oj_train)
table(poly_train, oj_train$Purchase)

#Test error rate

poly_test <- predict(oj_poly, oj_test)
table(poly_test, oj_test$Purchase)

(poly_error<- (107+37)/(452+107+37+204))
(ploye_error <- (46+15)/(149+46+15+60))
```
__**The training error rate is 18% amd the test error rate is 22.59%.**__

```{r}
#For optimal cost

poly_tune <- tune(svm, Purchase ~ ., data = oj_train, kernel = "polynomial", degree = 2, ranges =
                  data.frame(cost = seq(0.01, 10, length.out = 25)))
summary(poly_tune)
```

__**The optimal cost here is 10.**__

```{r}
#Training error rate
poly_oj <- svm(Purchase ~ ., data = oj_train, kernel = "polynomial", cost = poly_tune$best.parameters$cost)
train_poly <- predict(poly_oj, oj_train)
table(train_poly, oj_train$Purchase)

(tr_err_poly <- (74+40)/(449+74+40+237))

#Test error rate
test_poly <- predict(poly_oj, oj_test)
table(test_poly, oj_test$Purchase)

(te_err_poly <- (37+17)/(147+37+17+69))
```

__**The training error rate is 14.25% and the test error rate is 20%.**__

#(h)

__**The support vector classifier or the SVM with linear kernel and with cost 3.75625 gives the best results in terms of the test error rate. It gives the smallest test error rate of 18.15%, among all the approaches.**__




